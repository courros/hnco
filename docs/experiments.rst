=============
 Experiments
=============

HNCO is distributed with several experiments to analyze, benchmark, or
tune black-box optimization algorithms.

Each (example) experiment is located in a separate directory found in
the directory ``experiments/examples/``. An experiment is described in
a json file called ``plan.json``. A Makefile runs the simulations and
generates the report. The command::

  make

takes care of everything. Sometimes we only need the simulations::

  make run

or only the report::

  make report

To clean all generated files, enter::

  make clean

All experiments can use GNU parallel to run the simulations in
parallel hence take advantage of multicore architectures. To use GNU
parallel, set the key ``parallel`` to ``true``.

There is also limited support for remote execution. A list of remote
hosts can be specified in ``plan.json`` with the key ``servers``. For
each server, a hostname (or ip address) must be given. The relative
working directories of servers must be identical. GNU parallel
connects to servers with ssh.

-------------
Affine OneMax
-------------

This experiment is a stress test for random search heuristics.

-------------------
Algorithm parameter
-------------------

This experiment is similar to the benchmark experiment but the same
algorithm is run with some parameter taking values in a given set. In
the example, the influence of the learning rate on the performance of
PBIL is studied.

---------------
Autocorrelation
---------------

The purpose of this experiment is to visualize empirical
autocorrelation functions of time series generated by random walks on
the hypercube and various fitness functions.

---------
Benchmark
---------

The purpose of this experiment is to compare the performance of a set
of algorithms applied to a set of functions with a fixed budget. Each
algorithm is run 20 times on each function. Algorithms are ranked
according to their median performance (quartiles are also
considered). They are ranked first per function then globally.

----
ECDF
----

The purpose of this experiment is to compute and plot the empirical
cumulative distribution function of the runtime for each
algorithm-function couple in a given set. Results are computed per
function then globally. This experiment partly follows the
experimental procedure of the COCO framework (see references).

------------------
Function parameter
------------------

The purpose of this experiment is to study the influence of a function
parameter on the performance of search algorithms.

------------
Lookup ratio
------------

The purpose of this experiment is to compare the cache lookup ratio of
a set of algorithms applied to a set of functions with a fixed
budget. It is designed after the benchmark experiment. A high lookup
ratio indicates that an algorithm often resamples already sampled bit
vectors.

-----------------
Maximum evolution
-----------------

The purpose of this experiment is to study the evolution of the
maximum found so far. Each algorithm is run only once on each
function. Time is expressed in terms of number of evaluations.

--------------------
Observable evolution
--------------------

The purpose of this experiment is to study the evolution of some
quantity available through the log function of a given set of
algorithms. In the example, the evolution of entropy in UMDA and PBIL
is studied. Each algorithm is run only once on each function. Time is
expressed in terms of number of iterations.

-------
Runtime
-------

The purpose of this experiment is to study the influence of some given
parameter (e.g. bit vector size) on the runtime of a set of algorithms
applied to a set of functions. The functions must have a known maximum
and the algorithms must be able to find it in finite time.

----------------
Walsh transforms
----------------

The purpose of this experiment is to visualize the Walsh transforms of
various functions in the library.
